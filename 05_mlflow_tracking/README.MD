ðŸ“‚ MLflow Tracking

ðŸ”¹ What is MLflow?

MLflow is an open-source MLOps platform that helps manage the end-to-end ML lifecycle:
	â€¢	Experiment tracking (log parameters, metrics, and artifacts)
	â€¢	Model registry (versioning and stage transitions)
	â€¢	Reproducibility (environments, dependencies)
	â€¢	Deployment (serve models locally or via REST API)

ðŸ”¹ Why do we need MLflow?

Without MLflow:
	â€¢	You might store experiment results manually (e.g., in spreadsheets).
	â€¢	Re-running experiments with the same parameters/environment is hard.
	â€¢	Collaboration is difficult (no central place to track models/metrics).
	â€¢	Version control for ML models is missing.

With MLflow:
	â€¢	Every run logs parameters, metrics, artifacts, and models.
	â€¢	You can compare experiments easily.
	â€¢	Models are versioned and promoted (e.g., Staging â†’ Production).
	â€¢	Deployment is as simple as:

ðŸ”¹ Key Components
	1.	MLflow Tracking â†’ Logs metrics, params, and artifacts.
	2.	MLflow Projects â†’ Packaged ML code with environment.
	3.	MLflow Models â†’ Format to save/load/serve models.
	4.	MLflow Registry â†’ Central hub for versioning + lifecycle.

â¸»

ðŸ”¹ Benefits

âœ… Centralized tracking of experiments
âœ… Better reproducibility (auto-logs env + deps)
âœ… Model versioning & governance
âœ… Easy deployment and serving
âœ… Works with multiple frameworks (Sklearn, PyTorch, TensorFlow, etc.)

ðŸ”¹ Typical Workflow
	1.	Train model â†’ log runs with MLflow
	2.	Compare runs in MLflow UI
	3.	Register best model in Model Registry
	4.	Promote â†’ Staging/Production
	5.	Serve model via REST API


python train_mlflow.py
mlflow ui --host 0.0.0.0 --port 5000
Promote it to Staging or Production
mlflow models serve -m "models:/IrisClassifier/Production" -p 1234
curl -X POST http://127.0.0.1:1234/invocations \
  -H "Content-Type: application/json" \
  -d @predict_request.json

