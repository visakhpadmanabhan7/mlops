What is W&B?

Weights & Biases is an MLOps tool for:
	â€¢	Experiment tracking
	â€¢	Hyperparameter sweeps
	â€¢	Collaboration (dashboards, reports)
	â€¢	Model management

Why W&B?

Without W&B:
	â€¢	Experiments are logged manually (hard to compare).
	â€¢	Sharing results across team is messy.

With W&B:
	â€¢	Centralized dashboard of runs.
	â€¢	Easy comparison of parameters/metrics.
	â€¢	Hyperparameter tuning at scale.

- pip install -r requirements.txt
- wandb login
- python train_wandb.py

ğŸ”¹ Key Difference in Practice
	â€¢	MLflow:
	â€¢	Run â†’ Compare runs â†’ Register model â†’ Deploy to prod.
	â€¢	Focus = operationalization.
	â€¢	W&B:
	â€¢	Run â†’ Visualize charts â†’ Share report â†’ Tune hyperparams.
	â€¢	Focus = collaboration + exploration.

â¸»

ğŸ”¹ Analogy
	â€¢	MLflow = like Docker + Jenkins â†’ package, version, deploy.
	â€¢	W&B = like Google Docs for ML experiments â†’ easy to share, comment, visualize.


ğŸ”¹ What are Hyperparameters?
	â€¢	Parameters you set before training, not learned from data.
	â€¢	Examples:
	â€¢	RandomForest â†’ n_estimators, max_depth
	â€¢	Neural nets â†’ learning_rate, batch_size, num_layers
	â€¢	XGBoost â†’ eta, max_depth, colsample_bytree

They strongly influence performance, but thereâ€™s no formula for the â€œbestâ€ values.

â¸»

ğŸ”¹ What is Hyperparameter Sweeping?

Itâ€™s the process of automatically training the model multiple times with different hyperparameter combinations to find the best-performing one.

Think of it as grid-searching or auto-tuning:
	â€¢	Try n_estimators=50, 100, 200
	â€¢	Try max_depth=3, 5, None
	â€¢	Evaluate each run â†’ pick the one with highest accuracy (or lowest loss).

â¸»

ğŸ”¹ Types of Sweeps
	1.	Grid Search â†’ try all possible combinations.
	    â€¢	    E.g., 3Ã—3 grid = 9 runs.
	    â€¢	Expensive but exhaustive.
	2.	Random Search â†’ pick random combinations.
	    â€¢	Cheaper, surprisingly effective.
	3.	Bayesian Optimization â†’ use past results to pick smarter next combinations.
	    â€¢	Efficient for large search spaces.
	4.	Hyperband / Population-based â†’ advanced methods for deep learning.

ğŸ”¹ The options
	1.	grid
	â€¢	Tries every possible combination of parameters.
	â€¢	Example:
	â€¢	n_estimators: [50, 100, 200]
	â€¢	max_depth: [5, 10]
	â€¢	â†’ 3 Ã— 2 = 6 runs total.
	â€¢	âœ… Exhaustive, good for small search spaces.
	â€¢	âŒ Expensive for large search spaces.

â¸»

	2.	random
	â€¢	Picks random combinations from parameter space.
	â€¢	Example: same grid as above â†’ instead of all 6, it might sample 4 random ones.
	â€¢	âœ… Efficient for large spaces.
	â€¢	âœ… Often finds good configs faster than grid.
	â€¢	âŒ Might miss the absolute best if unlucky.

â¸»

	3.	bayes (Bayesian optimization)
	â€¢	Uses results of previous runs to guide next hyperparameter choice.
	â€¢	Smarter than random, especially with continuous values (e.g., learning rate).
	â€¢	âœ… More sample-efficient.
	â€¢	âŒ Slower per-step (needs optimization overhead).


GitHub Actions Workflow
	â€¢	Added .github/workflows/wandb-sweep.yml.
	â€¢	Triggers:
        â€¢	On push to wandb branch.
        â€¢	Manual dispatch (play button in Actions tab).
	â€¢	Steps:
        1.	Checkout repo.
        2.	Setup Python (3.11).
        3.	Install dependencies.
        4.	Run W&B sweep agent.
	â€¢	Configurable workflow inputs:
        â€¢	count â†’ number of runs (default: 20).
        â€¢	dataset â†’ wine or iris.
        â€¢	sweep_file â†’ path to sweep config file.

5. Authentication
	â€¢	W&B API key stored securely in GitHub Secrets (WANDB_API_KEY).

â¸»

ğŸ”¹ Workflow in Action
	1.	Push to wandb branch or manually trigger workflow.
	2.	GitHub runner launches sweep agent.
	3.	Each run:
        â€¢	Trains model with different hyperparams.
        â€¢	Logs metrics (accuracy, precision, recall, f1_score).
	4.	Results visible on wandb.ai dashboard.

ğŸ”¹ Workflow in Action
	1.	Trigger the workflow
        â€¢	Push to 06_weights_biases branch (dev mode)
        â€¢	Or manually trigger via Actions tab â†’ Run workflow (choose branch & inputs).
	2.	GitHub runner launches W&B sweep agent
        â€¢	Chooses project based on branch:
        â€¢	main â†’ wine-quality-prod
        â€¢	feature branch â†’ wine-quality-dev
	3.	Each run in the sweep
        â€¢	Trains a model with different hyperparameters from sweep.yaml.
        â€¢	Logs metrics to W&B:
        â€¢	Accuracy âœ…
        â€¢	Precision âœ…
        â€¢	Recall âœ…
        â€¢	F1-score âœ…
        â€¢	Tagged with GitHub metadata (branch + commit SHA).
        4.	View results in W&B dashboard
        â€¢	Dev runs and Prod runs stay separated.
        â€¢	Easy comparison of hyperparams across multiple runs.
        â€¢	Traceability: every run is linked back to the GitHub branch/commit.